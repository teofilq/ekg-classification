{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import wfdb\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "current_dir = Path.cwd()  \n",
    "data_dir = current_dir / 'data' / 'WFDBRecords'\n",
    "\n",
    "LEADS = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
    "SAMPLE_RATE = 500  \n",
    "NUM_SAMPLES = 5000  \n",
    "NUM_LEADS = 12    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_all_record_folders(data_dir):\n",
    "    \"\"\"Read main RECORDS file and return all folder paths\"\"\"\n",
    "    folders = []\n",
    "    with open(data_dir / 'RECORDS', 'r') as f:\n",
    "        folders = [line.strip() for line in f if line.strip()]\n",
    "    return folders\n",
    "\n",
    "def read_folder_records(folder_path):\n",
    "    \"\"\"Read RECORDS file from a specific folder\"\"\"\n",
    "    records = []\n",
    "    records_file = folder_path / 'RECORDS'\n",
    "    if records_file.exists():\n",
    "        with open(records_file, 'r') as f:\n",
    "            records = [line.strip() for line in f if line.strip()]\n",
    "    return records\n",
    "\n",
    "def read_subfolder_records(data_dir, main_folder, subfolder):\n",
    "    \"\"\"Read records from a specific subfolder\"\"\"\n",
    "    folder_path = data_dir / main_folder / subfolder\n",
    "    records = []\n",
    "    records_file = folder_path / 'RECORDS'\n",
    "    if records_file.exists():\n",
    "        with open(records_file, 'r') as f:\n",
    "            records = [line.strip() for line in f if line.strip()]\n",
    "    return records\n",
    "\n",
    "def read_header_metadata(header_file):\n",
    "    \"\"\"Read only essential metadata from .hea file\"\"\"\n",
    "    metadata = {\n",
    "        'name': None,          # JS00001\n",
    "        'checks': [],          # List of (offset, checksum) tuples\n",
    "        'age': None,          # Just the value\n",
    "        'sex': None,          # Just the value\n",
    "        'dx': None            # Just the value\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with open(header_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            \n",
    "            # First line: get just the name (rest is generic)\n",
    "            metadata['name'] = lines[0].split()[0]\n",
    "            \n",
    "            # Next 12 lines: get just offset and checksum\n",
    "            for line in lines[1:13]:\n",
    "                parts = line.strip().split()\n",
    "                metadata['checks'].append((int(parts[4]), int(parts[5])))\n",
    "                \n",
    "            # Get only needed patient info\n",
    "            for line in lines[13:]:\n",
    "                if line.startswith('#'):\n",
    "                    key, value = line[1:].strip().split(':', 1)\n",
    "                    key = key.lower()\n",
    "                    if key in ['age', 'sex', 'dx']:\n",
    "                        metadata[key] = value.strip()\n",
    "                        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading header file {header_file}: {e}\")\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "def load_record(record_path, data_dir, verify=True):\n",
    "    \"\"\"Load a single record with minimal metadata\"\"\"\n",
    "    base_path = str(record_path.parent / record_path.stem)\n",
    "    \n",
    "    try:\n",
    "        # Read the record using wfdb\n",
    "        record = wfdb.rdrecord(base_path, \n",
    "                             pn_dir=None, \n",
    "                             return_res=16)\n",
    "        \n",
    "        # Read minimal header metadata\n",
    "        header_file = Path(base_path + '.hea')\n",
    "        metadata = read_header_metadata(header_file)\n",
    "        \n",
    "        return {\n",
    "            'name': metadata['name'],\n",
    "            'data': record.p_signal,\n",
    "            'metadata': metadata\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {record_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def scan_dataset(data_dir):\n",
    "    \"\"\"Scan dataset and return a mapping of all available records\"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    dataset_map = {}\n",
    "    \n",
    "    folders = get_all_record_folders(data_dir)\n",
    "    for folder in folders:\n",
    "        folder_path = data_dir / folder\n",
    "        if not folder_path.exists():\n",
    "            continue\n",
    "            \n",
    "        records = read_folder_records(folder_path)\n",
    "        dataset_map[folder] = records\n",
    "        \n",
    "    return dataset_map\n",
    "\n",
    "def create_record_filter(folder=None, record=None):\n",
    "    \"\"\"Create a filter function for record selection\"\"\"\n",
    "    def filter_func(record_info):\n",
    "        folder_match = folder is None or record_info['folder'] == folder\n",
    "        record_match = record is None or record_info['record'] == record\n",
    "        return folder_match and record_match\n",
    "    return filter_func\n",
    "\n",
    "def load_records(data_dir, record_filter=None, verify=True, save_batch=1000):\n",
    "    \"\"\"Generic record loader with filtering\"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    \n",
    "    dataset_map = scan_dataset(data_dir)\n",
    "    dataset = {\n",
    "        'records': {},\n",
    "        'metadata': {\n",
    "            'total_records': 0,\n",
    "            'failed_records': [],\n",
    "            'verification_enabled': verify\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    records_to_load = []\n",
    "    \n",
    "    # Build list of records based on filter\n",
    "    for folder, records in dataset_map.items():\n",
    "        for record in records:\n",
    "            record_info = {'folder': folder, 'record': record}\n",
    "            if record_filter is None or record_filter(record_info):\n",
    "                records_to_load.append((folder, record))\n",
    "    \n",
    "    # Load filtered records with progress bar\n",
    "    with tqdm(total=len(records_to_load)) as pbar:\n",
    "        for folder, record in records_to_load:\n",
    "            record_path = data_dir / folder / record\n",
    "            data = load_record(record_path, data_dir, verify)\n",
    "            \n",
    "            if data is not None:\n",
    "                dataset['records'][data['name']] = data\n",
    "                pbar.update(1)\n",
    "                pbar.set_description(f\"Loaded {record} from {folder}\")\n",
    "            else:\n",
    "                dataset['metadata']['failed_records'].append(str(record_path))\n",
    "    \n",
    "    dataset['metadata']['total_records'] = len(dataset['records'])\n",
    "    return dataset\n",
    "\n",
    "def load_batch(data_dir, main_folder, subfolder, verify=True):\n",
    "    \"\"\"Load all records from a specific batch folder (e.g., 01/010)\"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    batch_path = data_dir / main_folder / subfolder\n",
    "    records = read_subfolder_records(data_dir, main_folder, subfolder)\n",
    "    \n",
    "    dataset = {\n",
    "        'records': {},\n",
    "        'metadata': {\n",
    "            'total_records': 0,\n",
    "            'failed_records': [],\n",
    "            'verification_enabled': verify,\n",
    "            'main_folder': main_folder,\n",
    "            'subfolder': subfolder\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"Loading batch from {main_folder}/{subfolder}\")\n",
    "    print(f\"Found {len(records)} records\")\n",
    "    \n",
    "    with tqdm(total=len(records)) as pbar:\n",
    "        for record_name in records:\n",
    "            record_path = batch_path / record_name\n",
    "            data = load_record(record_path, data_dir, verify)\n",
    "            \n",
    "            if data is not None:\n",
    "                dataset['records'][record_name] = data\n",
    "                pbar.update(1)\n",
    "                pbar.set_description(f\"Loaded {record_name}\")\n",
    "            else:\n",
    "                dataset['metadata']['failed_records'].append(str(record_path))\n",
    "    \n",
    "    dataset['metadata']['total_records'] = len(dataset['records'])\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /Users/teofil/Dev/GitHub/ekg-classification-pipeline\n",
      "Loading batch from 01/010\n",
      "Found 100 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded JS00104: 100%|██████████| 100/100 [00:00<00:00, 443.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch Summary:\n",
      "--------------------------------------------------\n",
      "Successfully loaded: 100 records\n",
      "Failed records: 0\n",
      "\n",
      "Record Details:\n",
      "Signal shape: (5000, 12)\n",
      "Sampling rate: 500 Hz\n",
      "\n",
      "Metadata:\n",
      "Age: 85\n",
      "Sex: Male\n",
      "Diagnosis: 164889003,59118001,164934002\n"
     ]
    }
   ],
   "source": [
    "\n",
    "current_dir = Path.cwd().parent  \n",
    "print(\"Current directory:\", current_dir)\n",
    "data_dir = current_dir / 'data' / 'WFDBRecords'\n",
    "# Load batch (01/010)\n",
    "batch_dataset = load_batch(data_dir, main_folder=\"01\", subfolder=\"010\")\n",
    "\n",
    "print(\"\\nBatch Summary:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Successfully loaded: {batch_dataset['metadata']['total_records']} records\")\n",
    "print(f\"Failed records: {len(batch_dataset['metadata']['failed_records'])}\")\n",
    "\n",
    "\n",
    "if batch_dataset['records']:\n",
    "    first_record = next(iter(batch_dataset['records'].values()))\n",
    "    print(\"\\nRecord Details:\")\n",
    "    print(f\"Signal shape: {first_record['data'].shape}\")\n",
    "    print(f\"Sampling rate: {SAMPLE_RATE} Hz\")  # Using constant\n",
    "    print(\"\\nMetadata:\")\n",
    "    print(f\"Age: {first_record['metadata']['age']}\")\n",
    "    print(f\"Sex: {first_record['metadata']['sex']}\")\n",
    "    print(f\"Diagnosis: {first_record['metadata']['dx']}\")\n",
    "\n",
    "    # Save both data and metadata\n",
    "    batch_data = np.stack([record['data'] for record in batch_dataset['records'].values()])\n",
    "    batch_metadata = {rid: {\n",
    "        'name': record['metadata']['name'],\n",
    "        'checks': record['metadata']['checks'],\n",
    "        'age': record['metadata']['age'],\n",
    "        'sex': record['metadata']['sex'],\n",
    "        'dx': record['metadata']['dx']\n",
    "    } for rid, record in batch_dataset['records'].items()}\n",
    "    \n",
    "\n",
    "\n",
    "    np.save(f'batch_{batch_dataset[\"metadata\"][\"main_folder\"]}_{batch_dataset[\"metadata\"][\"subfolder\"]}_data.npy', batch_data)\n",
    "    np.save(f'batch_{batch_dataset[\"metadata\"][\"main_folder\"]}_{batch_dataset[\"metadata\"][\"subfolder\"]}_metadata.npy', batch_metadata)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
