{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import neurokit2 as nk\n",
    "import mne\n",
    "import os\n",
    "import hashlib\n",
    "import wfdb\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def read_sha256sums(filepath):\n",
    "    \"\"\"Read SHA256SUMS.txt into a dictionary\"\"\"\n",
    "    sha256sums = {}\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            hash_value, filename = line.strip().split()\n",
    "            sha256sums[filename] = hash_value\n",
    "    return sha256sums\n",
    "\n",
    "def calculate_sha256(filepath):\n",
    "    \"\"\"Calculate SHA256 hash of a file\"\"\"\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        # Read file in chunks for memory efficiency\n",
    "        for byte_block in iter(lambda: f.read(4096), b\"\"):\n",
    "            sha256_hash.update(byte_block)\n",
    "    return sha256_hash.hexdigest()\n",
    "\n",
    "def verify_file_integrity(filepath, sha256sums, data_dir):\n",
    "    \"\"\"Verify a file's integrity against SHA256SUMS\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        return False, \"File does not exist\"\n",
    "    \n",
    "    calculated_hash = calculate_sha256(filepath)\n",
    "    relative_path = str(Path(filepath).relative_to(data_dir))\n",
    "    expected_hash = sha256sums.get(relative_path)\n",
    "    \n",
    "    if expected_hash is None:\n",
    "        return False, \"File not found in SHA256SUMS\"\n",
    "        \n",
    "    return calculated_hash == expected_hash, calculated_hash\n",
    "\n",
    "def get_all_record_folders(data_dir):\n",
    "    \"\"\"Read main RECORDS file and return all folder paths\"\"\"\n",
    "    folders = []\n",
    "    with open(data_dir / 'RECORDS', 'r') as f:\n",
    "        folders = [line.strip() for line in f if line.strip()]\n",
    "    return folders\n",
    "\n",
    "def read_folder_records(folder_path):\n",
    "    \"\"\"Read RECORDS file from a specific folder\"\"\"\n",
    "    records = []\n",
    "    records_file = folder_path / 'RECORDS'\n",
    "    if records_file.exists():\n",
    "        with open(records_file, 'r') as f:\n",
    "            records = [line.strip() for line in f if line.strip()]\n",
    "    return records\n",
    "\n",
    "def load_record(record_path, sha256sums, data_dir, verify=True):\n",
    "    \"\"\"Load a single record and optionally verify its integrity\"\"\"\n",
    "    base_path = str(record_path.parent / record_path.stem)\n",
    "    \n",
    "    # Files to check\n",
    "    hea_path = base_path + '.hea'\n",
    "    mat_path = base_path + '.mat'\n",
    "    \n",
    "    # Verify files if requested\n",
    "    if verify:\n",
    "        hea_ok, hea_hash = verify_file_integrity(hea_path, sha256sums, data_dir)\n",
    "        mat_ok, mat_hash = verify_file_integrity(mat_path, sha256sums, data_dir)\n",
    "        \n",
    "        if not (hea_ok and mat_ok):\n",
    "            print(f\"Validation failed for {record_path}\")\n",
    "            return None\n",
    "    \n",
    "    try:\n",
    "        # Read the record using wfdb\n",
    "        record = wfdb.rdrecord(base_path)\n",
    "        return {\n",
    "            'data': record.p_signal,\n",
    "            'fs': record.fs,\n",
    "            'sig_name': record.sig_name,\n",
    "            'units': record.units,\n",
    "            'baseline': record.baseline,\n",
    "            'adc_gain': record.adc_gain,\n",
    "            'record_name': record_path.stem\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {record_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_dataset(data_dir, max_records=None, verify=True):\n",
    "    \"\"\"\n",
    "    Load entire dataset with progress bar\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_dir : Path\n",
    "        Path to dataset root directory\n",
    "    max_records : int, optional\n",
    "        Maximum number of records to load (for testing)\n",
    "    verify : bool\n",
    "        Whether to verify file integrity\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing loaded records and metadata\n",
    "    \"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    \n",
    "    # Read SHA256SUMS if verification is requested\n",
    "    sha256sums = read_sha256sums(data_dir / 'SHA256SUMS.txt') if verify else {}\n",
    "    \n",
    "    # Get all folders\n",
    "    folders = get_all_record_folders(data_dir)\n",
    "    \n",
    "    # Store loaded records\n",
    "    dataset = {\n",
    "        'records': {},\n",
    "        'metadata': {\n",
    "            'total_records': 0,\n",
    "            'failed_records': [],\n",
    "            'verification_enabled': verify\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    records_loaded = 0\n",
    "    \n",
    "    # Process each folder with progress bar\n",
    "    with tqdm(total=max_records if max_records else None) as pbar:\n",
    "        for folder in folders:\n",
    "            folder_path = data_dir / folder\n",
    "            if not folder_path.exists():\n",
    "                continue\n",
    "                \n",
    "            records = read_folder_records(folder_path)\n",
    "            \n",
    "            for record in records:\n",
    "                if max_records and records_loaded >= max_records:\n",
    "                    break\n",
    "                    \n",
    "                record_path = folder_path / record\n",
    "                data = load_record(record_path, sha256sums, data_dir, verify)\n",
    "                \n",
    "                if data is not None:\n",
    "                    dataset['records'][data['record_name']] = data\n",
    "                    records_loaded += 1\n",
    "                    pbar.update(1)\n",
    "                    pbar.set_description(f\"Loaded {records_loaded} records\")\n",
    "                else:\n",
    "                    dataset['metadata']['failed_records'].append(str(record_path))\n",
    "            \n",
    "            if max_records and records_loaded >= max_records:\n",
    "                break\n",
    "    \n",
    "    dataset['metadata']['total_records'] = records_loaded\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data loading...\n",
      "This might take a while depending on your system...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded 1111 records: : 1111it [00:01, 574.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading /Users/teofil/Dev/GitHub/ekg-classification-pipeline/data/a-large-scale-12-lead-electrocardiogram-database-for-arrhythmia-study-1.0.0/WFDBRecords/01/019/JS01052: time data '/' does not match format '%d/%m/%Y'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded 22787 records: : 22787it [00:41, 550.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading /Users/teofil/Dev/GitHub/ekg-classification-pipeline/data/a-large-scale-12-lead-electrocardiogram-database-for-arrhythmia-study-1.0.0/WFDBRecords/23/236/JS23074: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded 45150 records: : 45150it [01:24, 532.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Summary:\n",
      "--------------------------------------------------\n",
      "Successfully loaded: 45150 records\n",
      "Failed records: 2\n",
      "\n",
      "Data Characteristics:\n",
      "--------------------------------------------------\n",
      "Signal shape per record: (5000, 12)\n",
      "Sampling rate: 500 Hz\n",
      "Channels: I, II, III, aVR, aVL, aVF, V1, V2, V3, V4, V5, V6\n",
      "\n",
      "Overall Statistics:\n",
      "--------------------------------------------------\n",
      "Total samples: (225750000, 12)\n",
      "Min value: nan\n",
      "Max value: nan\n",
      "Mean value: nan\n",
      "Std deviation: nan\n",
      "\n",
      "Saving dataset to numpy file...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "2709000000 requested and 1879047680 written",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStd deviation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mstd(all_data)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSaving dataset to numpy file...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfull_dataset.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dev/GitHub/ekg-classification-pipeline/.venv/lib/python3.13/site-packages/numpy/lib/_npyio_impl.py:581\u001b[0m, in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m file_ctx \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[1;32m    580\u001b[0m     arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(arr)\n\u001b[0;32m--> 581\u001b[0m     \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfix_imports\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfix_imports\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dev/GitHub/ekg-classification-pipeline/.venv/lib/python3.13/site-packages/numpy/lib/format.py:754\u001b[0m, in \u001b[0;36mwrite_array\u001b[0;34m(fp, array, version, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isfileobj(fp):\n\u001b[0;32m--> 754\u001b[0m         \u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtofile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    755\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    756\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m numpy\u001b[38;5;241m.\u001b[39mnditer(\n\u001b[1;32m    757\u001b[0m                 array, flags\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexternal_loop\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuffered\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzerosize_ok\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    758\u001b[0m                 buffersize\u001b[38;5;241m=\u001b[39mbuffersize, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[0;31mOSError\u001b[0m: 2709000000 requested and 1879047680 written"
     ]
    }
   ],
   "source": [
    "\n",
    "data_dir = Path('/Users/teofil/Dev/GitHub/ekg-classification-pipeline/data/a-large-scale-12-lead-electrocardiogram-database-for-arrhythmia-study-1.0.0')\n",
    "\n",
    "# Add estimate of total records for progress bar\n",
    "total_records = 10000  # Estimat - se va ajusta automat\n",
    "\n",
    "print(\"Starting data loading...\")\n",
    "print(\"This might take a while depending on your system...\")\n",
    "\n",
    "dataset = load_dataset(\n",
    "    data_dir=data_dir,\n",
    "    max_records=None,  \n",
    "    verify=True  \n",
    ")\n",
    "\n",
    "print(\"\\nDataset Summary:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Successfully loaded: {dataset['metadata']['total_records']} records\")\n",
    "\n",
    "if dataset['metadata']['failed_records']:\n",
    "    print(f\"Failed records: {len(dataset['metadata']['failed_records'])}\")\n",
    "    # Optional: save failed records list\n",
    "    with open('failed_records.txt', 'w') as f:\n",
    "        for record in dataset['metadata']['failed_records']:\n",
    "            f.write(f\"{record}\\n\")\n",
    "\n",
    "print(\"\\nData Characteristics:\")\n",
    "print(\"-\" * 50)\n",
    "if dataset['records']:\n",
    "    first_record = next(iter(dataset['records'].values()))\n",
    "    print(f\"Signal shape per record: {first_record['data'].shape}\")\n",
    "    print(f\"Sampling rate: {first_record['fs']} Hz\")\n",
    "    print(f\"Channels: {', '.join(first_record['sig_name'])}\")\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    all_data = np.vstack([record['data'] for record in dataset['records'].values()])\n",
    "    print(\"\\nOverall Statistics:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Total samples: {all_data.shape}\")\n",
    "    print(f\"Min value: {np.min(all_data):.2f}\")\n",
    "    print(f\"Max value: {np.max(all_data):.2f}\")\n",
    "    print(f\"Mean value: {np.mean(all_data):.2f}\")\n",
    "    print(f\"Std deviation: {np.std(all_data):.2f}\")\n",
    "    \n",
    "    print(\"\\nSaving dataset to numpy file...\")\n",
    "    np.save('full_dataset.npy', all_data)\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
